{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f457df4-a840-4838-9b30-fc5feb2c09b5",
   "metadata": {},
   "source": [
    "# Assignment 3: Neural networks in natural language processing\n",
    "\n",
    "### Due Date: Oct 30 (both sections)\n",
    "\n",
    "### Grade (100 pts, 10%)\n",
    "\n",
    "#### Your Name:\n",
    "\n",
    "#### Your EID:\n",
    "\n",
    "*Note: This assignment covers material from the recording, notes, demo, and suggested readings from Lecture-08*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5040c21a-f813-4ce5-b5d7-bbdd59e37dcc",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369615c-54d7-4a72-bfe2-6fd84cef9582",
   "metadata": {},
   "source": [
    "### 1. Dropout (50 pts)\n",
    "\n",
    "Dropout is a regularization technique that randomly sets units in each activation layer, $a \\in \\mathbb{R}^{D}$, to zero and then multiplies the resultant vector elementwise by a constant $\\gamma$ according to:\n",
    "\n",
    "$$a_{dropout} \\leftarrow  \\gamma H \\odot a$$\n",
    "\n",
    "where $\\odot$ represents the element-wise product operator and $H \\in \\{0, 1\\}^D$ is a mask with entries drawn from \n",
    "\n",
    "$$\\begin{cases} p(0) &= p_{dropout} \\\\ p(1) &= 1 - p_{dropout} \\end{cases}$$\n",
    "\n",
    "Select a scaling factor ${\\gamma}$ that ensures the expected value over the activation layer remains invariant to the above operation, $E\\big[ a_{dropout} \\big] = E\\big[ a \\big]$, and provide rationale for your selection.\n",
    "\n",
    "*Hint: You want to show that*\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^D a_i = \\gamma \\sum_{i=1}^D a_{dropout, i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d08cbe0-e13b-4325-a3b8-363c78a5fdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average estimated scaling factor for each value of p\n",
      "P:  0.0\n",
      "\t1/(1-p):  1.0\n",
      "\tGamma:  1.0\n",
      "P:  0.1\n",
      "\t1/(1-p):  1.1111111111111112\n",
      "\tGamma:  1.1109693886685041\n",
      "P:  0.2\n",
      "\t1/(1-p):  1.25\n",
      "\tGamma:  1.2500664578933312\n",
      "P:  0.3\n",
      "\t1/(1-p):  1.4285714285714286\n",
      "\tGamma:  1.4280009706105377\n",
      "P:  0.4\n",
      "\t1/(1-p):  1.6666666666666667\n",
      "\tGamma:  1.666721001199416\n",
      "P:  0.5\n",
      "\t1/(1-p):  2.0\n",
      "\tGamma:  2.0008511050356224\n",
      "P:  0.6\n",
      "\t1/(1-p):  2.5\n",
      "\tGamma:  2.4988443213182854\n",
      "P:  0.7\n",
      "\t1/(1-p):  3.333333333333333\n",
      "\tGamma:  3.334257964853906\n",
      "P:  0.8\n",
      "\t1/(1-p):  5.000000000000001\n",
      "\tGamma:  4.994702730783005\n",
      "P:  0.9\n",
      "\t1/(1-p):  10.000000000000002\n",
      "\tGamma:  10.003381229897204\n",
      "\n",
      "Based on this Monte Carlo-esque simulation, it seems that gamma is roughly = 1/(1-p) if gamma = mean[a]/mean[a_dropout]\n"
     ]
    }
   ],
   "source": [
    "# Your answer goes here\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "size = 10000\n",
    "\n",
    "gamma_values = {}\n",
    "\n",
    "for p in range(0, 100, 10):\n",
    "    gamma_values[p] = []\n",
    "    for i in range(1000):\n",
    "        # Initial array\n",
    "        a = np.random.rand(size)\n",
    "        \n",
    "        # H: 1s and 0s the same length as a, in proportion based on p\n",
    "        dropouts = np.random.choice([0, 1], size=size, p=[p/100, 1-(p/100)])\n",
    "        \n",
    "        # a after random dropouts\n",
    "        a_dropout = np.multiply(a, dropouts)\n",
    "\n",
    "\n",
    "        gamma = np.mean(a) / np.mean(a_dropout)\n",
    "        gamma_values[p].append(gamma)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Average estimated scaling factor for each value of p\")\n",
    "for k, v in gamma_values.items():\n",
    "    p = k/100\n",
    "    print(\"P: \", p)\n",
    "    print(\"\\t1/(1-p): \", 1/(1 - p))\n",
    "    print(\"\\tGamma: \", np.mean(v))\n",
    "\n",
    "print(\"\\nBased on this Monte Carlo-esque simulation, it seems that gamma is roughly = 1/(1-p) if gamma = mean[a]/mean[a_dropout]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2785529-1d06-4474-8aec-baef38545787",
   "metadata": {},
   "source": [
    "### 2. Convolutions (50 pts)\n",
    "\n",
    "Consider a sequence of $T$ token embeddings, $Z \\in \\mathbb{R}^{T \\times D}$, for which $D=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347fcde5-c66a-4b4d-a7de-8491b5fcc513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Z = np.array([\n",
    "    [1.3,   0.4, -0.2],\n",
    "    [-3.1,  1.1,  2.1],\n",
    "    [0.9,   2.8, -1.5],\n",
    "    [1.3,   2.4,  0.1],\n",
    "    [1.0,   1.0,  0.5],\n",
    "    [3.0,  -1.4, -0.2],\n",
    "    [-0.7,  1.8,  1.3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3cbe3-5b3a-44aa-886f-9f56e126e04d",
   "metadata": {},
   "source": [
    "and a set of convolutional filters, $W=\\{ w^{(1)}, w^{(2)} \\}$, and corresponding filter widths $S=\\{ s^{(1)}, s^{(2)}  \\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437d0a4d-57ea-42c9-97d1-60055b3fbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "w2 = np.array([\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2],\n",
    "    [2, 2, 2]\n",
    "])\n",
    "\n",
    "W = [w1, w2]\n",
    "\n",
    "S = [2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c6909-6c25-4dde-8a87-fa79b27dbf3e",
   "metadata": {},
   "source": [
    "In Lecture 08 we discussed a set of operations that maps $Z \\in \\mathbb{R}^{T \\times D}$ onto $Z' \\in \\mathbb{R}^{N_F D}$ (in this problem $N_F = 2$). This involved three steps:\n",
    "\n",
    "1. **Convolution**: The convolutional operation produces $N_F$ feature maps, $B^{(n)} \\in \\mathbb{R}^{(T - s^{(n)} + 1) \\times D}$, where $n=\\{1, \\dots, N_F\\}$, according to:\n",
    "\n",
    "$$\n",
    "\\forall_{t \\in \\{ 1, \\dots, T - s^{(n)} + 1 \\} } \\; B^{(n)}_{t,j} = \\sum_{t'=1}^{S^{(n)}} w^{(n)}_{t',j} \\; Z_{t+t'-1, \\ j}\n",
    "$$\n",
    "\n",
    "2. **Max pooling**: The max pooling operation computes the max over the sequence dimension in each feature map, $ B_{maxpool}^{(n)} \\in \\mathbb{R}^D$, according to:\n",
    "\n",
    "$$\n",
    "B_{maxpool, j}^{(n)} = \\underset{1 \\leq t' \\leq T - s^{(n)} + 1 }{\\max} B^{(n)}_{t', j}\n",
    "$$\n",
    "\n",
    "3. **Concatenation**: The resultant set of $N_F$ feature vectors are then concatenated into a single vector $Z'$ according to:\n",
    "\n",
    "$$\n",
    "Z' = \\big[ B_{maxpool}^{(1)}, \\dots, B_{maxpool}^{(n)}, \\dots,  B_{maxpool}^{(N_F)}  \\big] \\in \\mathbb{R}^{D \\cdot N_F}\n",
    "$$\n",
    "\n",
    "In the cell below, perform these three operations to produce $Z' \\in \\mathbb{R}^6$ and print it.\n",
    "\n",
    "*Hint: The max pooling operation computes the maximum over each column in $B^{(n)}$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7747a3-da78-44e9-ae19-c3f464bd9a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1:  [[ 1.7  1.5  0.2]\n",
      " [-0.3  1.6  3.4]\n",
      " [ 1.7  2.3  4.5]\n",
      " [ 7.4  6.   3.8]\n",
      " [ 5.7  6.3  4. ]\n",
      " [ 3.6  3.9 -0.1]\n",
      " [ 2.7  3.8  1.5]\n",
      " [ 1.1  2.4  3.1]]\n",
      "B2:  [[-0.6  3.2  6.8]\n",
      " [ 6.8  7.6  9.4]\n",
      " [10.8 12.2 14. ]\n",
      " [18.8 17.  10.6]\n",
      " [14.6 15.4  4.8]\n",
      " [ 9.4 12.6  6. ]\n",
      " [ 5.4  7.6  3. ]]\n",
      "B1 Max:  [7.4 6.3 4.5]\n",
      "B2 Max:  [18.8 17.  14. ]\n",
      "Z':  [ 7.4  6.3  4.5 18.8 17.  14. ]\n"
     ]
    }
   ],
   "source": [
    "# Assume 1 padding?\n",
    "Z_padded = np.pad(Z, 1)\n",
    "\n",
    "z_size = Z_padded.shape\n",
    "w1_size = w1.shape\n",
    "w2_size = w2.shape\n",
    "conv_feature_1_size = ((z_size[0] - w1_size[0]) + 1, (z_size[1] - w1_size[1]) + 1)\n",
    "conv_feature_2_size = ((z_size[0] - w2_size[0]) + 1, (z_size[1] - w2_size[1]) + 1)\n",
    "\n",
    "conv_1 = np.zeros(conv_feature_1_size).astype(np.float32)\n",
    "conv_2 = np.zeros(conv_feature_2_size).astype(np.float32)\n",
    "\n",
    "# Step 1: Convolution\n",
    "# Didn't use S for this -- it was easier to think about by manually calculating the \n",
    "for t in range(conv_feature_1_size[0]):\n",
    "    for j in range(conv_feature_1_size[1]):\n",
    "        conv_1[t][j] = np.sum(Z_padded[t:t + w1_size[0], j:j + w1_size[1]] * w1)\n",
    "\n",
    "print(\"B1: \", conv_1)\n",
    "\n",
    "for t in range(conv_feature_2_size[0]):\n",
    "    for j in range(conv_feature_2_size[1]):\n",
    "        conv_2[t][j] = np.sum(Z_padded[t:t + w2_size[0], j:j + w2_size[1]] * w2)\n",
    "\n",
    "print(\"B2: \", conv_2)\n",
    "\n",
    "# Step 2: Max Pooling\n",
    "\n",
    "b1_max = np.amax(conv_1, axis=0)\n",
    "b2_max = np.amax(conv_2, axis=0)\n",
    "print(\"B1 Max: \", b1_max)\n",
    "print(\"B2 Max: \", b2_max)\n",
    "\n",
    "# Step 3: Concatenation\n",
    "\n",
    "Z_prime = np.concatenate((b1_max, b2_max))\n",
    "print(\"Z': \", Z_prime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
